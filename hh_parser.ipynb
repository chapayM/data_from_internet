{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e227a3e",
   "metadata": {},
   "source": [
    "В файл вынесены функции помогающие парсить вакансии с сайта hh.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e10ef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e3b8ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция запичи страницы с вакансия с сайта hh.ru\n",
    "\n",
    "def page_for_parsing(search_text, num_page):\n",
    "\n",
    "    url = 'https://hh.ru/search/vacancy'\n",
    "\n",
    "    headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36',\n",
    "            }\n",
    "\n",
    "    params = {'text': search_text, 'page': num_page}\n",
    "\n",
    "    return requests.get(url=url, headers=headers, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "875d7a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция определения страниц в поиске\n",
    "\n",
    "def max_find_page(button_whith_number_find_pages):\n",
    "    return max([int(el) for el in button_whith_number_find_pages if el.isdigit()])           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e806d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция определения минимальной и максимальной зарплаты\n",
    "\n",
    "def min_max_salary(salary_data):\n",
    "    \n",
    "    currency_str=salary_data\n",
    "    salary_data.replace(\" \", \"\")\n",
    "    min_salary, max_salary = '', ''\n",
    "    \n",
    "    if salary_data[0].isdigit()or salary_data[0]=='о':\n",
    "        j = 1\n",
    "    elif salary_data[0]=='д':\n",
    "        j = 2\n",
    "    \n",
    "    for el in salary_data:\n",
    "        if j==1 and el.isdigit():\n",
    "            min_salary += el\n",
    "        elif el=='–':\n",
    "            j = 2\n",
    "        if j==2 and el.isdigit():\n",
    "            max_salary += el\n",
    "            \n",
    "    if min_salary == '':\n",
    "        min_salary = None\n",
    "    else:\n",
    "        min_salary = float(min_salary)\n",
    "        \n",
    "    if max_salary == '':\n",
    "        max_salary = None\n",
    "    else:\n",
    "        max_salary = float(max_salary)\n",
    "        \n",
    "    currency = currency_str[currency_str.rfind(\" \")+1:]\n",
    "    \n",
    "    return [min_salary, max_salary, currency]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cac0d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция формирования данных по вакансии\n",
    "\n",
    "def vacancy_data(information, search_text):\n",
    "    vacancy_name = information.find('a', {'class':'serp-item__title'}).text\n",
    "    vacancy_url = information.find('a', {'class':'serp-item__title'}, href=True)['href']\n",
    "    try:\n",
    "        salary_data = information.find('span', {'class':'bloko-header-section-3'}).text\n",
    "    except:\n",
    "        salary_data = None\n",
    "    try:\n",
    "        employer = information.find('a', {'class':'bloko-link bloko-link_kind-tertiary'}).text\n",
    "    except:\n",
    "        employer = None\n",
    "    try:\n",
    "        employer_url = 'https://hh.ru'+information.find('a', {'class':'bloko-link bloko-link_kind-tertiary'}, href=True)['href']\n",
    "    except:\n",
    "        employer_url = None\n",
    "    try:\n",
    "        adress = information.find('div', {'data-qa':'vacancy-serp__vacancy-address'}).text\n",
    "    except:\n",
    "        employer_url = None\n",
    "    try:\n",
    "        short_description_of_vacancy = (information.find('div', {'data-qa':'vacancy-serp__vacancy_snippet_responsibility'}).text +' '\n",
    "                                    +information.find('div', {'data-qa':'vacancy-serp__vacancy_snippet_requirement'}).text)\n",
    "    except:\n",
    "        short_description_of_vacancy = None\n",
    "        \n",
    "    if salary_data==None:\n",
    "        salary_list = [None, None, None]\n",
    "    else:\n",
    "        salary_list = min_max_salary(salary_data)\n",
    "    \n",
    "    vacancy_dict = {\n",
    "        'search_name': search_text,\n",
    "        'date_search': datetime.now(),\n",
    "        'hh_site': True,\n",
    "        'vacancy_name':vacancy_name,\n",
    "        'vacancy_url':vacancy_url, \n",
    "        'min_salary': salary_list[0],\n",
    "        'max_salary': salary_list[1],\n",
    "        'currency': salary_list[2],\n",
    "        'employer': employer,\n",
    "        'employer_url':employer_url,\n",
    "        'adress': adress,\n",
    "        'short_description_of_vacancy': short_description_of_vacancy}\n",
    "    \n",
    "    return vacancy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "710b85f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def hh_search_parsing_db_writing(search_text, db):\n",
    "\n",
    "    first_search_page = page_for_parsing(search_text, \"0\")\n",
    "\n",
    "    first_soup = bs(first_search_page.content, 'html.parser')\n",
    "    first_dom = html.fromstring(first_search_page.text)\n",
    "    \n",
    "    button_whith_number_find_pages = first_dom.xpath(\"//a[@class='bloko-button']/span/text()\")\n",
    "    max_page = max_find_page(button_whith_number_find_pages)\n",
    "    \n",
    "    vacancy_infromation_soup = first_soup.find_all('div', {'class':'serp-item'})\n",
    "    \n",
    "\n",
    "    for el in vacancy_infromation_soup:\n",
    "        db.hh.insert_one(vacancy_data(el, search_text))\n",
    "        \n",
    "    \n",
    "\n",
    "    for num_page in range(1, max_page):\n",
    "        sleep(0.5)\n",
    "        search_page = page_for_parsing(search_text, num_page)\n",
    "        soup = bs(search_page.content, 'html.parser')\n",
    "        vacancy_infromation_soup = soup.find_all('div', {'class':'serp-item'})\n",
    "        for el in vacancy_infromation_soup:\n",
    "             db.hh.insert_one(vacancy_data(el, search_text))\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
